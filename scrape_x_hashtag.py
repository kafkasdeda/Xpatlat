from playwright.sync_api import sync_playwright
import json
import asyncio
from playwright.async_api import async_playwright

# ğŸ” Ayarlanabilir Parametreler
HASHTAG = "%22jon jones%22%20filter%3Aimages%20min_faves%3A400%20lang%3Atr&src=typed_query4"       
MAX_TWEETS = 10            # En fazla kaÃ§ tweet Ã§ekilsin
COOKIE_FILE = "twitter_cookies.json"  # Daha Ã¶nce giriÅŸ yapÄ±larak kaydedilmiÅŸ cookie'ler

async def scrape():
    tweets = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)  # Headless=True: tarayÄ±cÄ± gÃ¶rÃ¼nmez
        context = await browser.new_context()

        # ğŸª Cookie'leri yÃ¼kle ve baÄŸlam (context) iÃ§ine ekle
        with open(COOKIE_FILE, "r") as f:
            cookies = json.load(f)
        await context.add_cookies(cookies)

        # ğŸ” Auth cookie ile sayfaya eriÅŸim
        page = await context.new_page()
        search_url = f"https://x.com/search?q={HASHTAG}"
        await page.goto(search_url)

        # â³ SayfanÄ±n yÃ¼klenmesini bekle
        await page.wait_for_timeout(5000)
        for _ in range(5):
            await page.mouse.wheel(0, 3000)
            await page.wait_for_timeout(3000)
        

        # ğŸ§¾ Tweet'leri sayfadan al (genelde dil bilgisi iÃ§eren div'ler gerÃ§ek tweet metnidir)
        tweet_elements = await page.query_selector_all("article div[lang]")

        for i, tweet in enumerate(tweet_elements):
            if i >= MAX_TWEETS:
                break
            try:
                text = await tweet.inner_text()
                tweets.append(text)
            except Exception as e:
                print(f"âš ï¸ Hata: {e}")

        await browser.close()

    # ğŸ‰ SonuÃ§larÄ± yazdÄ±r
    print(f"\nğŸ“¦ Toplam {len(tweets)} tweet bulundu:")
    for i, t in enumerate(tweets, 1):
        print(f"\n{i}. {t}")

if __name__ == "__main__":
    asyncio.run(scrape())